/*   Copyright 2009 Music and Entertainment Technology Laboratory - Drexel University      Licensed under the Apache License, Version 2.0 (the "License");   you may not use this file except in compliance with the License.   You may obtain a copy of the License at       http://www.apache.org/licenses/LICENSE-2.0   Unless required by applicable law or agreed to in writing, software   distributed under the License is distributed on an "AS IS" BASIS,   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.   See the License for the specific language governing permissions and   limitations under the License.*/package{		import flash.utils.ByteArray;	import flash.events.SampleDataEvent;	import flash.events.IOErrorEvent;	import flash.events.ProgressEvent;	import flash.media.Sound;	import flash.media.SoundChannel;		import flash.net.*;	import flash.events.Event;	import flash.utils.Endian;	import flash.events.IOErrorEvent;	import flash.events.EventDispatcher;	import flash.utils.*;	import flash.errors.IOError;	import DATF;		/*		Class: ALF.as				The Audio processing Library for Flash. ALF plays an audio file (MP3 or wav) using the dynamic audio functionality 		of Actionscript3 and provides methods for obtaining audio features and manipulating the audio output stream on a frame by frame basis.	*/	public class ALF extends EventDispatcher{				// Sound playback				var audio:Sound;		var mp3Audio:Sound;		var audioCh:SoundChannel;		var mp3Bytes:ByteArray;		var playBytes:ByteArray;		var loadedData:URLLoader;		public var numCh:uint;		public var fs:uint;				private var leftSample:Number;		private var rightSample:Number;				private var mp3Position:uint = 0;		private var wavPosition:uint;		private var fileExt:String;						private var waveLoader:URLLoader;		private var mp3Loader:URLLoader;		private var STEREO:Boolean = false;		private var READY:Boolean = false;		private var STOP:Boolean = false;		private var CONTINUE:Boolean = false;				public var ITER:uint= 0;		public var LOAD_COUNT:uint = 0;				// Framing		public var chVal:Number;		public var numSamplesToPlay:Number;		private var userFrameRate:uint = 0;		public var frameSize:uint = 4096;		public var hopSize:uint = 2048;		// DATF		private var DSP:DATF;				//C Memory stuff		private var cRAM:ByteArray;		private var audioPtrs:Array;		private const sizeofFloat:Number = 4;				//Buffer Status vars		private var leftBufferStatus:Array;		private	var rightBufferStatus:Array;		private	var numSamplesLeft:int;		private var numSamplesRight:int;				// Events		public const NEW_FRAME:String = "newFrame";		public const FILE_LOADED:String = "audioFileLoaded";		public const FILE_COMPLETE:String = "audioFileCompleted";		public const PROG_EVENT:String = "newProgressEvent";		public const URL_ERROR:String = "urlErrorEvent";				// Utility		private var i:uint;		private var tempInt:int;			private var tempNum:Number;		private var verbose:Boolean;		public var loadProgress:Number;		//publicly available so audio file-loading can be monitored outside of the class		private var INITIALIZED:Boolean = false;				// Features		public var inten:Number, cent:Number, band:Number, roll:Number, flux:Number;		public var harmAmps:Array, harmFreqs:Array, magArr:Array, fftArr:Array;		/*									Topic: Usage						ALF is a library whose functions return audio features and perform other operations on an audio file 	on a frame by 			frame basis. In this current version, an event, NEW_FRAME, is dispatched whenever a new audio frame begins. To synchronize 			visual events with audio, use NEW_FRAME as you might use ENTER_FRAME (event that signals a new frame in the 			timeline). A simple example that plays a file and plots a feature is shown below:						Topic: Example						This is a simple example that loads an audio file into ALF and then plots one of the audio features as the song plays. The			values are calculated in real time and then displayed on the stage. On each frame, a line segment is drawn from the feature			value of the previous frame to the feature value of the current frame.						(start code)			public class ALFDemo extends MovieClip{								private var myALF:ALF;								private var intensity:Number;								// Display				var vidFrame:uint = 0;				var line:MovieClip;				var colorChange:ColorTransform;				var lineArr:Array;				var xCoord:uint = 0;						var val:Number;								// Utilities				var i:uint = 0;				var count:uint = 0;				var frameCount:uint = 1;				var offset:uint = 2;																		public function ALFDemo(){										// Define audio file, use this example file or specify the path (local or server) of your own file					var str:String;					str = 'http://schubert.ece.drexel.edu/~raym/sound_files/SA1.wav';							// Create ALF object					myALF = new ALF(str, 20, false);								// 20 fps 					myALF.addEventListener(myALF.NEW_FRAME, onFrame);				// Audio callback (ALF functions should be called in this handler)					myALF.addEventListener(myALF.FILE_LOADED, audioLoaded);			// Adds listener for when the audio data has loaded					myALF.addEventListener(myALF.FILE_COMPLETE, audioFinished);		// Event for when the file has fiished playing										addEventListener(Event.ENTER_FRAME, onEnterFrame);				// Video playback is based on this 											// Initialize objects for drawing					lineArr = new Array();					line = new MovieClip();					lineArr.push(line);					line.graphics.lineStyle( 1, 0xFF0000, 1000);					line.graphics.moveTo(0, 400);					addChild(line);															}						// This handles the event that ALF dispatches for each audio frame. If your video frame rate is				// the same, you should have synchronicity between your audio feature values and your video frames,				// that is, there should be no lag or offset between the value calculated and the audio that is playing.								public function onFrame(event:Event):void{										intensity = myALF.getIntensity();				}								// This funciton is called when the audio has been loaded in ALF and the FILE_LOADED event has been dispatched				public function audioLoaded(event:Event):void{																myALF.startAudio();					trace('playing audio ...');				}										// This funciton is called when the audio has finished playing and the FILE_COMPLETE event has been dispatched				public function audioFinished(event:Event):void{										trace('audioFinished'); 					trace('---------------------------------------');				}								// This draws the line based on the intensity value on every frame				protected function onEnterFrame(event:Event):void{										// Clear screen if reached border					if(xCoord > 550){								for(i = 0; i < lineArr.length; i++){							lineArr[i].graphics.clear();						}						line.graphics.moveTo(offset, 400);						xCoord = 0;					}																if(frameCount > offset){															// Draw line											val = intensity/10						if(isNaN(val)){ val = 0;} 						line.graphics.lineStyle( 1, 0xFF0000, 1000);						line.graphics.lineTo(xCoord, 400 - val);									addChild(line);																	// Set up for draw on next frame						line = new MovieClip();						lineArr.push(line);						line.graphics.moveTo(xCoord, 400 - val);									}																			frameCount++;					xCoord = xCoord + 3;									}								}			(end)						*/		/*			Group: Constructor						Constructor: ALF						Constructor for ALF.						Parameters:							filename - The filename of the audio file to play. This must be a .wav or .mp3 file.				framesPerSecond - The frame rate in frames per second. Values between 10 and 40 fps are currently supported. 				_verbose - When true, prints the .wav file header in the output window. The header contains information such as the 						   sample rate, bit rate, number of channels, size, etc. This parameter does not effect mp3 files.							Notes:							The frame rate that audio is processed at is based on the NEW_FRAME (<ALF Events>) event. Make sure to set the frame 				rate of your .fla file to the same rate you enter as the framesPerSecond parameter if you want good synchronization 				between audio and video.		*/		public function ALF(filename:String, framesPerSecond:uint, _verbose:Boolean){					// FrameSize is set after the sound file has been loaded in			userFrameRate = framesPerSecond;			trace('fps = '+userFrameRate);								// Save parameter values			verbose = _verbose;						// Initialize the sound objects			audio = new Sound();			audioCh = new SoundChannel();									// Find file extension			fileExt = filename.substr(filename.length - 3, 4);			fileExt = fileExt.toLowerCase();						switch(fileExt){								case "wav": 	// Initialize wav objects								waveLoader = new URLLoader();											// Initialize URL Looader															waveLoader.dataFormat = URLLoaderDataFormat.BINARY;						// Set to binary format								waveLoader.addEventListener(Event.COMPLETE, waveLoaded);				// Event for when file is loaded								waveLoader.addEventListener(ProgressEvent.PROGRESS, waveProgress);		// Event for getting file loading data								waveLoader.addEventListener(IOErrorEvent.IO_ERROR, waveLoadError);		// Event for file loading erros								audio.addEventListener(SampleDataEvent.SAMPLE_DATA, wavAudioCallback); 	// Add event for each audio frame								var waveRequest:URLRequest = new URLRequest(filename);					// Set filename to load													// Attempting to read in the wave file								waveLoader.load(waveRequest);								break;								case "mp3":		// Initialize mp3 objects								var mp3Request:URLRequest = new URLRequest(filename);					// Load filename								mp3Bytes = new ByteArray();												// For raw samples from file																mp3Bytes.endian = Endian.LITTLE_ENDIAN;									// Set endianess for data processed in CPP								playBytes = new ByteArray();											// For playback of endian converted samples								playBytes.endian = Endian.BIG_ENDIAN;									// Set endianess for playback in Flash																mp3Audio = new Sound();													// Sound object for audio file								STEREO = true;								fs = 44100;																// Attempting to read in the MP3 file								mp3Audio.addEventListener(IOErrorEvent.IO_ERROR, mp3LoadError);			// Handle failed load								mp3Audio.addEventListener(Event.COMPLETE, mp3Loaded);					// Dispatched when file is loaded									mp3Audio.addEventListener(ProgressEvent.PROGRESS, mp3Progress);								mp3Audio.load(mp3Request);																			audio.addEventListener(SampleDataEvent.SAMPLE_DATA, mp3AudioCallback); 	// Dispatched on each audio frame																				break;								default:		trace('Invalid file type! ALF only supports .wav and .mp3 files');			}								trace('loaded in the audio file....');		}				// These functions notify the user of an error in loading the audio file		private function mp3LoadError(error:IOErrorEvent):void{			trace('Error loading mp3: '+error);			dispatchEvent(new Event(URL_ERROR));		}				private function waveLoadError(error:IOErrorEvent):void{			trace('Error loading wav file: '+error);			dispatchEvent(new Event(URL_ERROR));		}				/******************************************************************************		*		*							AUDIO PLAYBACK FUNCTIONS		*		******************************************************************************/						// This is an internal function to play the current file again automatically when the end of the file is reached.		// If the boolean CONTINUE is true, then the file will loop, if it is false, the file will only be played once.		private function continueAudio():void{						switch(fileExt){								case 	"wav": 	loadedData.data.position = 44;										// Reset to beginning of file								audioCh = audio.play();												// Start playback								audioCh.addEventListener(Event.SOUND_COMPLETE, audioComplete);								break;										case	"mp3":	mp3Position = 0;													// Reset to beginning of file 								mp3Bytes.length = 0;												// Reset byte array								mp3Position += mp3Audio.extract(mp3Bytes,hopSize,mp3Position);		// Extract audio								audioCh = audio.play();												// Start playback								audioCh.addEventListener(Event.SOUND_COMPLETE, audioComplete);										break;			}					}				/*			Group: Audio Playback							Function: loadNewSong						Use this function to load a new file into ALF for processing and playback.						Parameters:						filename - The filename (.wav or .mp3) of the audio file to be played. A new FILE_LOADED event will be dispatched					   when the file has finished loading.		*/		public function loadNewSong(filename:String):void{						// Sets the flag that this is a new file being loaded				DSP.endOfFile();						// Re-initialize the sound objects			audio = null;			audioCh = null;			audio = new Sound();			audioCh = new SoundChannel();												// Find file extension			fileExt = filename.substr(filename.length - 3, 4);			fileExt = fileExt.toLowerCase();									switch(fileExt){								case "wav": 	// Initialize wav objects								waveLoader = null;								waveLoader = new URLLoader();											// Initialize URL Looader															waveLoader.dataFormat = URLLoaderDataFormat.BINARY;						// Set to binary format								waveLoader.addEventListener(Event.COMPLETE, waveLoaded);				// Event for when file is loaded								waveLoader.addEventListener(ProgressEvent.PROGRESS, waveProgress);		// Event for getting file loading data								waveLoader.addEventListener(IOErrorEvent.IO_ERROR, waveLoadError);		// Event for file loading erros								audio.addEventListener(SampleDataEvent.SAMPLE_DATA, wavAudioCallback); 	// Add event for each audio frame								var waveRequest:URLRequest = new URLRequest(filename);					// Set filename to load													// Attempting to read in the wave file								waveLoader.load(waveRequest);								break;								case "mp3":		// Initialize mp3 objects								var mp3Request:URLRequest = new URLRequest(filename);					// Load filename								mp3Bytes = new ByteArray();												// For raw samples from file																mp3Bytes.endian = Endian.LITTLE_ENDIAN;									// Set endianess for data processed in CPP								playBytes = new ByteArray();											// For playback of endian converted samples								playBytes.endian = Endian.BIG_ENDIAN;									// Set endianess for playback in Flash																mp3Audio = new Sound();													// Sound object for audio file								STEREO = true;								fs = 44100;																// Attempting to read in the MP3 file								mp3Audio.addEventListener(IOErrorEvent.IO_ERROR, mp3LoadError);			// Handle failed load								mp3Audio.addEventListener(Event.COMPLETE, mp3Loaded);					// Dispatched when file is loaded									mp3Audio.addEventListener(ProgressEvent.PROGRESS, mp3Progress);								mp3Audio.load(mp3Request);																			audio.addEventListener(SampleDataEvent.SAMPLE_DATA, mp3AudioCallback); 	// Dispatched on each audio frame																				break;								default:		trace('Invalid file type! ALF only supports .wav and .mp3 files');			}											}				/*					Function: startAudio						A function to begin playback of an audio file. The FILE_LOADED event (<ALF Events>) must have been dispatched 			and received by the class instantiating ALF prior to calling startAudio. Use startAudio to resume playback after			calling pauseAudio.						See Also:						<stopAudio()>,				<pauseAudio()>,				<Example>		*/		public function startAudio():void{ 											// When audio has already been played then stopped.			if(READY && STOP){								switch(fileExt){										case 	"wav":  audioCh = audio.play();									audio.addEventListener(SampleDataEvent.SAMPLE_DATA, wavAudioCallback);																		audioCh.addEventListener(Event.SOUND_COMPLETE, audioComplete);									break;												case	"mp3":	audioCh = audio.play();									audio.addEventListener(SampleDataEvent.SAMPLE_DATA, mp3AudioCallback);																		audioCh.addEventListener(Event.SOUND_COMPLETE, audioComplete);									break;				}															}			// First time being played or continuing after a pause 			else if (READY){ 				audioCh = audio.play();				audioCh.addEventListener(Event.SOUND_COMPLETE, audioComplete);							}else if(!READY){				trace('ALF not initialized. Do not call startAudio() until file has loaded.');			}						STOP = false;		}				/*			Function: pauseAudio						A function to pause playback of an audio file. To resume playback, use startAudio.						See Also:						<startAudio()>,				<stopAudio()>		*/				public function pauseAudio():void{			audioCh.stop();					}				/*			Function: stopAudio						A function to stop playback of an audio file. This function removes the SampleDataEvent associated with the audio object. 			The object will no longer call for more data to playback. Note that the SOUND_COMPLETE event will not be dispatched if			this function is called. When startAudio is called after stopAudio, the current file loaded will play from the beginning.						See Also:						<startAudio()>,				<pauseAudio()>,				<Example>		*/		public function stopAudio():void{						//DSP.clearAudioBuffers();			DSP.endOfFile();						switch(fileExt){								case 	"wav":  audioCh.stop();							// Stopping playback																loadedData.data.position = 44;			// Reset to beginning of file																break;										case	"mp3":	audioCh.stop();							// Stopping playback																			mp3Bytes.length = 0;					// Reset ByteArray								mp3Position = 0;						// Reset to beginning of file								mp3Position += mp3Audio.extract(mp3Bytes,hopSize,mp3Position);																break;			}						STOP = true;		}						/******************************************************************************		*		*									EVENTS				*		*******************************************************************************/		// These functions dispatch the PROG_EVENT to monitor the progress of a file 		// being loaded. View the documentation for more information.		private function waveProgress(evt:ProgressEvent):void {			loadProgress = evt.bytesLoaded/evt.bytesTotal;			dispatchEvent(new Event(PROG_EVENT));		}		private function mp3Progress(evt:ProgressEvent):void {			loadProgress = evt.bytesLoaded/evt.bytesTotal;			dispatchEvent(new Event(PROG_EVENT));		}					// This function initializes sets all parameters in the C Library 		private function waveLoaded(evt:Event):void {						loadedData = URLLoader(evt.target);		// Establishing reference to object containing data read in from URLRequest			extractWaveHeader(loadedData);			// Extract wave file header								// Need to set the frameSize based on desired userFrameRate and fs from the audio			if(userFrameRate > 40 || userFrameRate < 10){				trace("Invalid user-selected frame rate. Valid frame rates are between 10fps and 40fps.");				trace("Defaulting to 10fps");				hopSize = 1024;			}else {				hopSize = Math.round(fs/userFrameRate);			}				frameSize = 2*hopSize;								// If this is the first file loaded into the DATF, we use the constructor, if not we must call reInitializeChannel.			// The program will crash if you attempt to create a new DATF after already having created one.			if(!INITIALIZED){				// Initialize the DATF				DSP = new DATF(hopSize, STEREO, fs);						DSP.addEventListener(DSP.PROCESS_FRAME, processFrameHandler);								// Get the Reference to the cRAM data				cRAM = DSP.getCRAM();				audioPtrs = DSP.getChOutPtrs();								INITIALIZED = true;			}else{								// Re-initializing the channel changes the frame sizes based on the new sampling rate and reallocates				// C memory accordingly				if(DSP.getHopSize() != hopSize || numCh != DSP.getNumberOfChannels()){									DSP.reInitializeChannel(hopSize, fs, numCh);		// New hopSize and fs are set in extractWaveHeader().				}			}			// Set ready for playback flag			READY = true;			dispatchEvent(new Event(FILE_LOADED));		}		// This function initializes sets all parameters in the C Library 		private function mp3Loaded(evt:Event):void{						// Stereo			numCh = 2;			STEREO = true;						// Need to set the hopSize based on desired userFrameRate and fs from the audio			if(userFrameRate > 40 || userFrameRate < 10){				trace("Invalid user-selected frame rate. Valid frame rates are between 10fps and 40fps.");				trace("Defaulting to 10fps");				hopSize = 1024;			}else {				hopSize = Math.round(fs/userFrameRate);			}							frameSize = 2*hopSize;			// If this is the first file loaded into the DATF, we use the constructor, if not we must call reInitializeChannel.			// The program will crash if you attempt to create a new DATF.			if(!INITIALIZED){				// Initialize the DATF				DSP = new DATF(hopSize, STEREO, fs);						DSP.addEventListener(DSP.PROCESS_FRAME, processFrameHandler);								// Get the Reference to the cRAM data				cRAM = DSP.getCRAM();				audioPtrs = DSP.getChOutPtrs();								INITIALIZED = true;			}else{								// Re-initializing the channel changes the frame sizes based on the new sampling rate and reallocates				// C memory accordingly												mp3Bytes.length = 0; 				if(DSP.getHopSize() != hopSize || DSP.getNumberOfChannels() != numCh){					DSP.reInitializeChannel(hopSize, fs, numCh);				}															}												// Extract samples of first frame						mp3Bytes.position = 0;			mp3Position += mp3Audio.extract(mp3Bytes, frameSize, mp3Position);															//Set ready for playback flag			READY = true;			dispatchEvent(new Event(FILE_LOADED));		}				// Dispatches the NEW_FRAME event		public function processFrameHandler(event:Event):void{			dispatchEvent(new Event(NEW_FRAME));					}						// Called when the file has completed		private function audioComplete(evt:Event):void{			//DSP.clearAudioBuffers();			DSP.endOfFile();						if(CONTINUE){				continueAudio();							}else{				switch(fileExt){										case "wav":	loadedData.data.position = 44; 											// Reset to beginning of file								break;													case "mp3": mp3Bytes.length = 0;													// Reset ByteArray								mp3Position = 0;														// Reset to beginning of file								mp3Position += mp3Audio.extract(mp3Bytes,hopSize,mp3Position);		// Extract samples																							break;				}			}			STOP = true;			dispatchEvent(new Event(FILE_COMPLETE));	// Event to tell user file has finished		}								// Plays wav audio samples		private function wavAudioCallback(evt:SampleDataEvent):void {			var bufferReady:Boolean = false;									// As long as there is data left to read into C...we do that and process the data			// by dispatching a new event.			if(loadedData.data.bytesAvailable != 0){				DSP.setFrame(loadedData.data, "short");			}						// Check the status of the buffers to see if they're ready for playback....			leftBufferStatus = DSP.checkOutputBuffer("left");			numSamplesToPlay = leftBufferStatus[1];			bufferReady = leftBufferStatus[0];			if(STEREO) {				rightBufferStatus  = DSP.checkOutputBuffer("right");					numSamplesToPlay = Math.min(leftBufferStatus[1], rightBufferStatus[1]);					if(rightBufferStatus[0] && leftBufferStatus[0]) {					bufferReady = true;				} else {					bufferReady = false;				}			}			// If not ready for playback, load another frame and check again			while(!bufferReady){								if(!(loadedData.data.bytesAvailable == 0)) {					DSP.setFrame(loadedData.data, "short");				}								leftBufferStatus = DSP.checkOutputBuffer("left");				numSamplesToPlay = leftBufferStatus[1];				bufferReady = leftBufferStatus[0];				if(STEREO) {					rightBufferStatus  = DSP.checkOutputBuffer("right");						numSamplesToPlay = Math.min(leftBufferStatus[1], rightBufferStatus[1]);						if(rightBufferStatus[0] && leftBufferStatus[0]) {						bufferReady = true;					} else {						bufferReady = false;					}				}			}			// Play the samples			if(STEREO){								switch(fs){										case 44100:		for(i = 0; i < numSamplesToPlay; i++){																				cRAM.position = audioPtrs[0] + i*sizeofFloat;		//position for leftCh										leftSample = cRAM.readFloat();										cRAM.position = audioPtrs[1] + i*sizeofFloat;		//position for rightCh										rightSample = cRAM.readFloat();																				// Write to output stream										evt.data.writeFloat(leftSample);										evt.data.writeFloat(rightSample);									}									break;														case 22050:		for(i = 0; i < numSamplesToPlay; i++){																				cRAM.position = audioPtrs[0] + i*sizeofFloat;		//position for leftCh										leftSample = cRAM.readFloat();										cRAM.position = audioPtrs[1] + i*sizeofFloat;		//position for rightCh										rightSample = cRAM.readFloat();																				// Write to output stream										evt.data.writeFloat(leftSample);										evt.data.writeFloat(rightSample);										evt.data.writeFloat(0);										evt.data.writeFloat(0);									}				}			}else if (!STEREO){								switch(fs){										case 44100: 	for(i = 0; i < numSamplesToPlay; i++){																				cRAM.position = audioPtrs[0] + i*sizeofFloat;		//position for leftCh										leftSample = cRAM.readFloat();																				// Write to output stream										evt.data.writeFloat(leftSample);										evt.data.writeFloat(leftSample);									}									break;														case 22050:		for(i = 0; i < numSamplesToPlay; i++){																				cRAM.position = audioPtrs[0] + i*sizeofFloat;		//position for leftCh										leftSample = cRAM.readFloat();																				// Write to output stream										evt.data.writeFloat(leftSample);										evt.data.writeFloat(leftSample);										evt.data.writeFloat(0);										evt.data.writeFloat(0);									}									break;				}			}else{				trace('ERROR: ALF can only handle mono or stereo files.');			}		}				// Plays mp3 audio samples		private function mp3AudioCallback(evt:SampleDataEvent):void {			var bufferReady:Boolean = false;									// As long as there is data left to read into C...we do that and process the data			// by dispatching a new event (in DATF)			if(mp3Bytes.length > 0){				mp3Bytes.position = 0;				DSP.setFrame(mp3Bytes, "float");			}																			// Check the status of the buffers to see if they're ready for playback....			leftBufferStatus = DSP.checkOutputBuffer("left");			rightBufferStatus  = DSP.checkOutputBuffer("right");												numSamplesToPlay = Math.min(leftBufferStatus[1], rightBufferStatus[1]);							if(rightBufferStatus[0] && leftBufferStatus[0]){ 				bufferReady = true;			}						//Extract audio data from sound object			mp3Bytes.length = 0;						mp3Position += mp3Audio.extract(mp3Bytes, hopSize, mp3Position);			// Extract samples											// If not ready for playback, load another frame and check again			while(!bufferReady){				if(mp3Bytes.length > 0){									mp3Bytes.position = 0;					DSP.setFrame(mp3Bytes, "float");				// Load the audio frame into the C buffer								}											// Check the status of the buffers to see if they're ready for playback....				leftBufferStatus = DSP.checkOutputBuffer("left");				rightBufferStatus  = DSP.checkOutputBuffer("right");													numSamplesToPlay = Math.min(leftBufferStatus[1], rightBufferStatus[1]);									if(rightBufferStatus[0] && leftBufferStatus[0]) {					bufferReady = true;				} else {					bufferReady = false;				}									//Extract audio data from sound object				mp3Bytes.length = 0;							mp3Position += mp3Audio.extract(mp3Bytes, hopSize, mp3Position);				}						for(i = 0; i < numSamplesToPlay; i++){														cRAM.position = audioPtrs[0] + i*sizeofFloat;		//position for leftCh				leftSample = cRAM.readFloat();				cRAM.position = audioPtrs[1] + i*sizeofFloat;		//position for rightCh				rightSample = cRAM.readFloat();								// Write samples				evt.data.writeFloat(leftSample);				evt.data.writeFloat(rightSample);			}																				}				/******************************************************************************		*		*						  DSP FUNCTIONS		*			******************************************************************************/					/*						Group: Audio Features						Values returned from all features are dependent upon the input audio. The style, genre, 			and instrumentation play a significant role in 	the numbers returned from each function 			but the production value (i.e. compression/mastering) also weighs heavily into what range			of values is returned, especially for the intensity.						Function: getIntensity						This function calculates the intensity of the current audio frame loaded into ALF. The 			intensity is a measure of how much energy is in the current	frame. If there are many 			instruments playing loudly, this value will be large, for an ambient section of a song,			this value will be smaller.						Returns:								An Number which is the intensity value for the current frame. The range of values 				will be dependent mostly upon the production value of the audio file. 						*/				public function getIntensity():Number{						inten = DSP.getIntensity();			return inten;		}				/*			Function: getBrightness			Brightness is an approximation of the timbre. If there is high frequency content, such as			a horn section, then the brightness is higher, for low frequency, such as drum and bass, the			brightness value will be lower.						Returns:						A Number which is the brightness (in Hz) value for the current frame. Typical values will 			be around several thousand hertz. The range of values will be between 0 and the sample rate 			divided by 2. For an MP3, the values will be from 0 - 22050. For a wav file, the sample frequency 			can be viewed in the .wav file 	header which can be printed out. For info on printing the 			.wav file header see <ALF>.		*/				public function getBrightness():Number{						cent = DSP.getCentroid();			return cent;		}						/*			Function: getFlux			This function calculates the change in frequency content for each frame. Instantaneous changes in 			the audio content (both new sounds and sudden quiet) will produce large flux values.						Returns:						A Number which is the flux value for the current frame.		*/				public function getFlux():Number{						flux = DSP.getFlux();			return flux;		}								/*			Function: getBandwidth					This function calculates the bandwidth of the current audio frame loaded into ALF. The 			bandwidth represents the range of frequencies present in the audio at the current frame.			This value gives an estimate of the instrumentation. A full band with drums, vocals, keys,			bass, guitar, synth, etc. will have a large bandwidth. An a solo cello performance will have			a smaller bandwidth.						Returns:								An Number which is the bandwidth (Hz) value for the current frame. Typical values 				will be around several thousand hertz.The range of values will be between 0 and the 				sample rate divided by 2. For an MP3, the values will be from 0 - 22050. For a wav 				file, the sample frequency can be viewed in the .wav file header which can be printed 				out. For info on printing the .wav file header see <ALF>.		*/			public function getBandwidth():Number{						band = DSP.getBandwidth();			return band;		}				/*			Function: getRolloff						This function calculates the frequency ceiling of the current frame. Rolloff is the frequency below which			most of the instruments lie.						Returns:								A Number which is the rolloff value for the current frame. The range of values will be 				between 0 and the sample rate divided by 2. For an MP3, the values will be from 0 - 22050. 				For a wav file, the sample frequency can be viewed in the wave file header which can be printed out.				For info on printing the .wav file header see <ALF>.						*/				public function getRolloff():Number{						roll = DSP.getRolloff();			return roll;		}				/*			Function: getSpectrum						This function calculates the magnitude of the frequency spectrum of the current frame.						Parameters:							fftSize - The number of DFT points used in performing the Fourier Transform. If you are				unfamiliar with the Fourier Transform use default (0), this provides optimezed computation 				and uses the next power of 2 greater than or equal to the frameSize.  						Returns:								An Array of spectral amplitudes of length (fftSize/2 + 1). Only half of the spectrum is returned 				since it is assumed that audio is the input signal and the magnitude spectrum is symmetric.						Notes:							This is the magnitude spectrum, only real values are returned.		*/			public function getSpectrum(fftSize:Number):Array{									magArr = [];			magArr = DSP.magSpectrum(fftSize);						return magArr;		}				/*			Function: getComplexSpectrum						This function calculates the complex frequency spectrum of the current frame.						Parameters:							fftSize - The number of points used in calculating the Discrete Fourier Transform. If you are				unfamiliar with the Fourier Transform use default (0), this is he quickest option and uses 				the next power of 2 greater than or equal to the frameSize.  						Returns:								An Array containing the complex valued Discrete Fourier Transform of the current frame. The Array is of the form								Re[0] Im[1] Re[2] Im[3] ... Re[N-2] Im[N - 1]					*/			public function getComplexSpectrum(fftSize:Number):Array{									fftArr = [];			fftArr = DSP.FFT(fftSize);						return fftArr;		}						/*			Function: getHarmonics						This function computes the partials assoicated with the audio spectrum						Returns:								Nothing. getHarmFreqs and getHarmAmps return the partial frequencies and amplitudes.							See Also:								<getHarmonicFrequencies()>,				<getHarmonicAmplitudes()>		*/				public function getHarmonics(numHarms:uint):void {			DSP.getHarmonics(numHarms);		}		/*			Function: getHarmonicFrequencies						Returns the harmoinc Frequencies generated from the getHarmonics call. getHarmonics must be called before			this.						Returns:								An Array of frequencies.							See Also:							<getHarmonicHarmonics()>,				<getHarmonicAmplitudes()>		*/						public function getHarmonicFrequencies():Array {					harmFreqs = DSP.getHarmonicFrequencies();			return harmFreqs;		}				/*			Function: getHarmonicAmplitudes						Returns the harmonics amplitudes generated from the getHarmonics call. getHarmoincs must be called before			this.						Returns:								An array of amplitudes.							See Also:							<getHarmonics()>,				<getHarmonicFrequencies()>		*/				public function getHarmonicAmplitudes():Array {			harmAmps = DSP.getHarmonicAmplitudes();			return harmAmps;		}				/*			Group: Audio Processing Functions					Function: reverb						This function adds reverb to the output stream.						Parameters:								* *activate* - A string value indicating whether reverb should be turned on or off.							on - adds reverb to the processing chain. It will persist unless it is turned off.							off - removes reverb from the processing chain. It will remain off until it is turned on again.											* *processCh* - An int specifying the channel reverb should be applied to					1 - applies it just to the left channel (default in DATF)					2 - applies it to both channels (if you're working with a Stereo file)														* *level* - 	A value from 1-4 (1 is the lowest, 4 is the highest) specifying the level of reverb to be applied.						-								* *roomType* - 	A string specifying the type of room to simulate. The sound source and listener locations assume								that the source is near the front of the room and the listener (mic) is at the center.																			small - simulates a room of dimensions 10x10x8 in feet						big - simulates a room of dimensions 20x20x8 in feet						hall - simulates a 'hall' of dimensions 30x50x30 in feet		*/		public function reverb(activate:String, processCh:int, level:uint, roomType:String):void {						var roomX:Number, roomY:Number, roomZ:Number, srcX:Number, srcY:Number, srcZ:Number,				micX:Number, micY:Number, micZ:Number;			var echoStrength:Number;						switch(level) {				case 1:					echoStrength = 0.25;					break;				case 2:					echoStrength = 0.50;					break;				case 3:					echoStrength = 0.75;					break;				case 4:					echoStrength = 1.00;					break;				default:					echoStrength = 0.0;					break;			}						switch(roomType) {				case "small":						roomX = 3; roomY = 3; roomZ = 2.4; //makes the room 10x10x8 in feet						srcX = 1.5, srcY = 1, srcZ = 1.5;						micX = 1.5, micY = 2, micZ = 1.5;					break;				case "big":						roomX = 6; roomY = 6; roomZ = 3; //makes the room 20x20x10 in feet						srcX = 3, srcY = 1, srcZ = 1.5;						micX = 3, micY = 4.5, micZ = 1.5;					break;				case "hall":						roomX = 9; roomY = 15; roomZ = 9; //makes the hall 30x50x30 in feet						srcX = 4.5, srcY = .5, srcZ = 1.5;						micX = 4.5, micY = 7.5, micZ = 1.5;					break;				default:						roomX = 3; roomY = 3; roomZ = 2.4; //makes the room 10x10x8 in feet						srcX = 1.5, srcY = 1, srcZ = 1.5;						micX = 1.5, micY = 2.8, micZ = 1.5;					break;			}						DSP.addReverb(activate, processCh, level, roomX, roomY, roomZ,							  srcX, srcY, srcZ, micX, micY, micZ);					}				// Un-documented function for full reverb access through ALF		public function reverbDemo(activate:String, processCh:int, level:Number,							   roomX:Number, roomY:Number, roomZ:Number,							   srcX:Number, srcY:Number, srcZ:Number,							   micX:Number, micY:Number, micZ:Number):void{						DSP.addReverb(activate, processCh, level,						  roomX, roomY, roomZ,						  srcX, srcY, srcZ,						  micX, micY, micZ);		}							// An internal function to extract the information from the .wav file header and set		// the sample rate and number of channels accordingly		private function extractWaveHeader(waveData:URLLoader):void{			var wfh_chunkID:String = waveData.data.readMultiByte(4,"utf");						waveData.data.endian = Endian.LITTLE_ENDIAN;			var wfh_chunkSize:uint = waveData.data.readUnsignedInt(); 			waveData.data.endian = Endian.BIG_ENDIAN;			var wfh_format:String = waveData.data.readMultiByte(4,"utf");			var wfh_subChunk1D:String = waveData.data.readMultiByte(4,"iso-8859-1");			waveData.data.endian = Endian.LITTLE_ENDIAN;				var wfh_subChunk1Size:int = waveData.data.readInt()			var wfh_audioFormat:int = waveData.data.readShort();			var wfh_channels:int = waveData.data.readShort();			var wfh_fs:int = waveData.data.readUnsignedInt();			var wfh_bytesPerSec:int = waveData.data.readUnsignedInt();			var wfh_blockAlign:int = waveData.data.readShort()			var wfh_bits:int = waveData.data.readUnsignedShort();			waveData.data.endian = Endian.BIG_ENDIAN;					var wfh_dataChunkSignature:String = waveData.data.readMultiByte(4,'iso-8859-1')			waveData.data.endian = Endian.LITTLE_ENDIAN;						var wfh_numBytes:int = waveData.data.readInt();			var wfh_numSamples:int = wfh_numBytes/(wfh_bits/8);						//set the number of samples per Channel			var wfh_numSamplesPerChannel = wfh_numSamples/wfh_channels;			/**************************** End Header Parsing **********************************/						// Display information extracted from the header if specified			if(verbose){							trace('----------------------------------------------');				trace('Wave File Information');				trace('ChunkID: '+ wfh_chunkID);				trace('ChunkSize: '+ wfh_chunkSize);				trace('Format: '+ wfh_format);							trace('SubChunk1D: '+ wfh_subChunk1D);				trace('SubChunk1Size: '+ wfh_subChunk1Size);							trace('Audio Format: '+ wfh_audioFormat);							trace('Channels: '+ wfh_channels);				trace('SampleRate: '+ wfh_fs);				trace('Bytes/Second: '+ wfh_bytesPerSec);							trace('BlockAlign: '+ wfh_blockAlign);				trace('Bits/Sample: '+ wfh_bits);							trace('Data Chunk Signature: '+ wfh_dataChunkSignature);							trace('Data Chunk Length: ' + wfh_numBytes + " bytes     " + wfh_numSamples + " samples");				trace('Byte position after reading header info: '+ waveData.data.position);				trace('----------------------------------------------\n');						}															fs = wfh_fs;								// Store sample frequency						// Set mono/stereo			if(wfh_channels == 2){ 										numCh = 2;				STEREO = true;			}					else{				 numCh = 1;				 STEREO = false};			if(!(fs == 22050 || fs == 44100)){trace('UNSUPPORTED SAMPLE RATE! ALF supports only 22kHz or 44.1kHz sample rates');}					}			}		/*		Group: Events			Topic: ALF Events				There are five events ALF dispatches that are essential to properly using the library.									* *FILE_LOADED* - 	This event is dispatched when the audio file has finished loading. No other function calls should be made after							the ALF object is created until *after* this event is received. For more information see 							http://livedocs.adobe.com/flash/9.0/ActionScriptLangRefV3/flash/media/Sound.html#event:complete.							Note that the load time will depend upon whether you are using Flash or AIR. AIR can access the local file system, 							therefore load times will be short. If using Flash, the time it takes for this listener to dispatch after the ALF 							constructor is called will vary significantly depending on filesize, filetype, and server/network speed. 										* *NEW_FRAME*  	-This event is sychronized with the SampleDataEvent in the Actionscript Sound class. For more information							see http://livedocs.adobe.com/flex/3/langref/flash/events/SampleDataEvent.html.										* *FILE_COMPLETE* - This event is dispatched when the audio file has finished playing. When there is no more data for the Sound object							to process (i.e. the last frame is reached) this event will be released.						* *PROG_EVENT* -	This is event is dispatched in accordance with the ProgressEvent incurred from a new							URLRequest for loading an audio file.			* *URL_ERROR* - This event signals that there was a problem loading the audio file.	*/}